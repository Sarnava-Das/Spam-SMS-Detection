{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pathlib\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_path = os.path.join(os.path.dirname(os.getcwd()),'parent','constants','__init__.py')\n",
    "\n",
    "\n",
    "spec = importlib.util.spec_from_file_location('__init__', source_file_path)\n",
    "source_file = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(source_file)\n",
    "\n",
    "\n",
    "path=[]\n",
    "for dirname, _, filenames in os.walk(os.path.join(os.path.dirname(os.getcwd()),source_file.DATASET_DIR,source_file.DATASET_PROCESSED_DIR)): \n",
    "    for filename in filenames:\n",
    "        if(pathlib.Path(os.path.join(dirname, filename)).suffix =='.csv'):\n",
    "           path.append(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the imported files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in path:\n",
    "    if(os.path.basename(filename)=='spam.csv'):\n",
    "        train_set=pd.read_csv(filename) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis(EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### At first get a brief idea of data i.e features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head(2)#gives first 2 rows of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gather insights of the data now i.e null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()  #gives datatype,count of entries i.e for checking null values in features \n",
    "print(\"\\n\",\"=\"*80,\"\\n\")\n",
    "test_set.info()\n",
    "print(\"\\n\",\"=\"*80,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find actual % of null values of all features now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives % of null values corresponding to all features\n",
    "print(100*train_set.isnull().sum()/len(train_set))\n",
    "print(\"\\n\",\"=\"*80,\"\\n\")\n",
    "print(100*test_set.isnull().sum()/len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get a better insight of data i.e mean,s.d,percentiles etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives a vivid insight of data i.e mean,count,max,min,std.50% etc\n",
    "print(train_set.describe())\n",
    "print(\"\\n\",\"=\"*80,\"\\n\")\n",
    "print(test_set.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make a copy of actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_mod= train_set.copy()\n",
    "test_set_mod= test_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a pandas DataFrame 'data' with a 'SUBDIVISION' column\n",
    "genre_counts = train_set_mod[\"GENRE\"].value_counts()\n",
    "\n",
    "# Create a bar chart\n",
    "genre_counts.plot(kind='barh', figsize=(10, 8))\n",
    "plt.xlabel(\"Count\", size=12)\n",
    "plt.ylabel(\"Genre\", size=12)\n",
    "plt.title(\"Count of Genre\")\n",
    "plt.grid(axis=\"x\", linestyle=\"-.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_categories=train_set_mod['GENRE'].unique()\n",
    "plt.figure(figsize=(8,8))\n",
    "values = train_set_mod['GENRE'].value_counts()\n",
    "plt.pie(values, labels=output_categories, autopct='%1.1f', startangle=90, radius=1.2, explode=(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.8, 0.75, 0.65, 0.55, 0.45, 0.35, 0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "genre_text = ' '.join(train_set_mod['GENRE'])\n",
    "movie_name_text = ' '.join(train_set_mod['TITLE'])\n",
    "\n",
    "\n",
    "genre_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(genre_text)\n",
    "movie_name_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(movie_name_text)\n",
    "\n",
    "# Display the word clouds\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(genre_wordcloud, interpolation='bilinear')\n",
    "plt.title('Genre Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(movie_name_wordcloud, interpolation='bilinear')\n",
    "plt.title('Movie Name Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now building a corpus which will be a 2d list with 27 rows one row for each genre's description corpus\n",
    "corpus = []\n",
    "for i in range(len(train_set_mod['GENRE'].unique())):\n",
    "    corpus_i = []\n",
    "    for desc in train_set_mod[train_set_mod['Labeled Genre'] == i]['Processed Description'].tolist():\n",
    "        for word in desc.split():\n",
    "            corpus_i.append(word)\n",
    "    corpus.append(corpus_i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=9, ncols=3,figsize=(16,30))\n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "fig.suptitle('Bar Graphs showing the Most Common words')\n",
    "\n",
    "for i, l in enumerate(corpus):\n",
    "    colors = np.random.rand(20, 3)\n",
    "    ax = axs[i//3,i%3]\n",
    "    df = pd.DataFrame(Counter(l).most_common(20))\n",
    "    ax.bar(df[0], df[1], color=colors)\n",
    "    ax.set_title(le.inverse_transform([i])[0])\n",
    "\n",
    "    # Setting tick positions and labels\n",
    "    ax.set_xticks(np.arange(len(df[0])))\n",
    "    ax.set_xticklabels(df[0].tolist(),rotation=90)\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
